This section presents the experimental study carried out to validate the performance of \DEEDM{}.
%
Specifically, we show that by explicitly controlling the diversity in \DE{}, results of state-of-the-art algorithms
are improved further.
%
Particularly, the benchmarks of \CEC{} 2016 and \CEC{} 2017 are considered.
%
Each one of them is composed of thirty different problems, meaning that the validation is performed with a set of large and diverse
functions.
%
The kind of problems of each benchmark (Table \ref{tab:category}) is divided in uni-modal , simple multi-modal, hybrid and composition functions.
%
Principally, considering that in the real-world optimization problems, different sub-components of the variables may have different properties.
%
In the hybrid functions, the variables are randomly divided into some sub-components and each one is related to basic functions.
%
In the same line, the composition functions merge the properties of several sub-functions and maintains continuity around each local optima.
%
Also the local optima which has the smallest bias value is the global optimum.
%
The search space is bounded by the range $\Omega = \prod_{j=1}^D[-100, 100]^D$.

\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Type Function} & \textbf{CEC 2016} & \textbf{CEC 2017} \\ \hline
\textbf{Uni-modal} & $f_1$ - $f_3$ & $f_1$ - $f_3$ \\ \hline
\textbf{Simple multi-modal} & $f_4$ - $f_{16}$ & $f_4$ - $f_{10}$ \\ \hline
\textbf{Hybrid} & $f_{17}$ - $f_{22}$ & $f_{11}$ - $f_{20}$ \\ \hline
\textbf{Composition} & $f_{23}$ - $f_{30}$ & $f_{21}$ - $f_{30}$ \\ \hline
\end{tabular}
\caption{Problems grouped by properties.}\label{tab:category}
\end{table}
%
The state-of-the-art is composed by the algorithms that attained the first places of each year competition.
%
Additionally, the standard \DE{} was included.
%
Thus, the algorithms considered from the \CEC{} 2016 are UMOEAs-II \cite{elsayed2016testing} and L-SHADE-EpSin \cite{awad2016ensemble}, which achieved 
the first and second places respectively.
%
Similarly, the top algorithms from \CEC{} 2017 are taken into account, i.e. EBOwithCMAR~\cite{kumar2017improving} and jSO~\cite{brest2017single}.
%
%It is interesting to remark that EBOwithCMAR is considered as an improvement of the UMOEAs-II.
%
%Additionally, jSO and L-SHADE-EpSin belong to the SHADE's family.
%
Following the recommendations given in~\cite{molina2017analysis}, all these algorithms are tested with both benchmarks.

Given that the optimizers taken into account are stochastic algorithms, each execution was repeated 51 times with different seeds.
%
In every case, the stopping criterion was set to $25 \times 10^6$ functions evaluations.
%
In addition, ten dimensions of the search space were take into account.
%
The validation follows the guidelines of \CEC{} benchmark competitions and the statistical tests proposed in~\cite{Joel:StatisticalTest} are also included.
%
%The first, considers a composed score between the mean of difference error and the rank of the algorithms.
%
%The second, follows a statistical scheme, which applies pair-comparisons based on the distribution of the error between the algorithms.
%
Note that, as it is usual in these competitions, when the gap between the values of the best solution found and the optimal solution is $10^{-8}$ or smaller, 
the error is treated as $0$.
%
%The minimal tolerance to consider a determined problem solved is $1e-8$, hence if the difference of the optimal obtained and the true optinal is below this tolerance then the error is zero.
%
The parameterization indicated by the authors was used in every algorithm and it is as follows:
\begin{itemize}
\item \textbf{EBOwithCMAR}: For EBO, the maximum population size of $S_1 = 18D$, minimum population size of $S_1 = 4$, maximum population size of $S_2 = 146.8D$, minimum population size of $S_2 = 10$, historical memory size H=$6$. For CMAR Population size $S_3 = 4 + 3log(D)$, $\sigma=0.3$, CS = $50$, probability of local search $pl = 0.1$ and $cfe_{ls} = 0.4* FE_{max}$.
\item \textbf{UMOEAs-II}: For MODE, maximum population size of $S_1 = 18D$, minimum population size of $S_1 = 4$, size memory H=$6$. For CMA-ES Population size $S_2 = 4 + \lfloor 3log(D) \rfloor$, $\mu=\frac{PS}{2}$, $\sigma=0.3$, CS = $50$. For local search, $cfe_{ls} = 0.2 * FE_{max}$.
\item \textbf{jSO}: Maximum population size = $25log(D)\sqrt{D}$, historical memory size H= $5$, initial mutation memory $M_F = 0.5$, initial probability memory $M_{CR} = 0.8$, minimum population size = $4$, initial p-best = $0.25*N$, final p-best = $2$.
\item \textbf{L-SHADE-EpSin}: Maximum population size = $25log(D)\sqrt{D}$, historical memory size H= $5$, initial mutation memory $M_F = 0.5$, initial probability memory $M_{CR} = 0.5$, initial memory frequency $\mu_F = 0.5$, minimum population size = $4$, initial p-best = $0.25*N$, final p-best = $2$, generations of local search $G_{LS}=250$.
\item \textbf{ DE-EDM}: $D_I = 0.3$, population size = $250$, CR $\sim Normal( \{0.2, 0.9\}, 0.1)$, F $\sim Cuachy(0.5, 0.5*nfes/max_{nfes})$.
\item \textbf{ Standard-DE}: population size = $250$ (operators as \DEEDM{}), CR $\sim Normal( \{0.2, 0.9\}, 0.1)$, F $\sim Cauchy(0.5, 0.5*nfes/max_{nfes})$.
\end{itemize}
%

Our experimental analyses is based on the error, i.e. the difference between the optimal solution and the best obtained solution.
%
In order to statistically compare the results, a similar guideline than the one proposed in~\cite{Joel:StatisticalTest} was used. 
%
First a Shapiro-Wilk test was performed to check whatever or not the values of the results followed a Gaussian distribution. 
%
If, so, the Levene test was used to check for the homogeneity of the variances. 
%
If samples had equal variance, an ANOVA test was done; if not, a Welch test was performed. 
%
For non-Gaussian distributions, the non parametric Kruskal-Wallis test was used to test whether samples are drawn from the same distribution. 
%
An algorithm $X$ is said to win algorithm $Y$ when the differences between them are statistically significant, and the mean and median error obtained by $X$ are lower 
than the mean and median achieved by $Y$.

Tables \ref{tab:Summary_CEC2016} and \ref{tab:Summary_CEC2017} offer a summary of the results obtained for \CEC{} 2016 and \CEC{} 2017, respectively.
%
The column tagged with ``Always Solved'' shows the number of functions where a zero error was obtained in the 51 runs.
%
Additionally, column tagged with ``At least one time solved'' shows the number of functions that were solved at least in one run.
%
Practically all functions (28 of them) of the \CEC{} 2017 benchmark were solved with \DEEDM{} at least one time.
%
Additionally, 21 functions of the \CEC{} 2016 were also solved.
%
This contrasts with the results obtained by state-of-the-art algorithms.
%
They were able to reach optimal values in significantly less functions.
%
In order to confirm the superiority of \DEEDM{}, the pair-wise statistical tests already described were used.
%
The column tagged with the symbol $\uparrow$ shows the number of cases where the superiority of each method could be confirmed, whereas
the column tagged with the symbol $\downarrow$ counts the number of cases where the method was inferior.
%
Finally, the number comparisons with not significant differences are shown in the column tagged with the symbol $\longleftrightarrow$.
%
The results of the statistical tests show that \DEEDM{} attained the best results in both years.
%
The number of victories in \CEC{} 2016 and \CEC{} 2017 were $77$ and $88$, whereas 
the number of losses were $25$ and $6$, respectively.
%
\DEEDM{} is the approach with the largest number of victories and lowest number of losses in both benchmarks, confirming the superiority
of the proposal.
%Additionally, the last place attained in both years was by the L-SHADE-Epsilon with $20$ wins in 2016 and $7$ wins in 2017.
%
%
The last column --- tagged with ``Score'' --- considers the official score of \CEC{}'s competitions.
%
Particularly, the raking of the algorithms is attained by taking into account the two scores defined in Eq. (\ref{eqn:total_scores}).
%
Then, the final score is calculated as the sum $Score = Score_1 + Score_2$.
%
In Eq.~(\ref{eqn:total_scores}), the $SE$ of an algorithm is the sum of the mean error values obtained in the $30$ benchmark functions, i.e. 
$SE = \sum_{i=1}^{30} error\_f_i$ .
%
Then, $SE_{min}$ is the minimal $SE$ from all the algorithms. 
%
In order to calculate $SR$ and $SR_{min}$, algorithms are sorted in each function in base of the attained mean error.
%
Then, a rank is assigned to each algorithm in base of such an ordering.
%
Finally, the $SR$ of a method is the sum of the ranks obtained for each function and $SR_{min}$ is the minimal $SR$ from all the algorithms.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
%\begin{scriptsize}
\centering
\caption{Summary results - \CEC{} 2016}
\label{tab:Summary_CEC2016}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Always \\ solved\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}At least one\\ time solved\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
 &  &  & $\uparrow$ & $\downarrow$ & $\longleftrightarrow $ &  \\ \hline
\textbf{DE-EDM} & 13 & 21 & 77 & 25 & 48 & 100.00 \\ \hline
\textbf{UMOEAs-II} & 9 & 14 & 51 & 31 & 68 & 62.45 \\ \hline
\textbf{Standard-DE} & 11 & 19 & 50 & 46 & 54 & 56.29 \\ \hline
\textbf{jSO} & 9 & 17 & 47 & 51 & 52 & 55.43 \\ \hline
\textbf{EBOwithCMAR} & 8 & 14 & 35 & 56 & 59 & 50.28 \\ \hline
\textbf{L-SHADE-Epsilon} & 7 & 13 & 20 & 71 & 59 & 50.12 \\ \hline
\end{tabular}
%\end{scriptsize}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
%\begin{scriptsize}
\centering
\caption{Summary results - \CEC{} 2017}
\label{tab:Summary_CEC2017}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Always \\ solved\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}At least one\\ time solved\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
 &  &  & $\uparrow$ & $\downarrow$ & $\longleftrightarrow $ &  \\ \hline
\textbf{DE-EDM} & 21 & 28 & 88 & 6 & 56 & 100.00 \\ \hline
\textbf{Standard-DE} & 12 & 21 & 56 & 29 & 65 & 42.91 \\ \hline
\textbf{EBOwithCMAR} & 9 & 18 & 34 & 46 & 70 & 37.14 \\ \hline
\textbf{L-SHADE-Epsilon} & 8 & 19 & 7 & 81 & 62 & 32.78 \\ \hline
\textbf{jSO} & 8 & 15 & 29 & 55 & 66 & 29.30 \\ \hline
\textbf{UMOEAs-II} & 11 & 15 & 43 & 40 & 67 & 26.89 \\ \hline
\end{tabular}
%\end{scriptsize}
\end{table}

\begin{equation}\label{eqn:total_scores}
\begin{split}
Score_1 &= \left (1 - \frac{SE - SE_{min}}{SE} \right) \times 50, \\
Score_2 &= \left  (1 - \frac{SR - SR_{min}}{SR} \right ) \times 50, \\
\end{split}
\end{equation}

Note that in base of this definition, the best attainable score is $100$.
%
This happens when a given approach obtains both $SR_{min}$ and $SE_{min}$.
%
\DEEDM{} attained the best attainable score in both years, which confirms its clear superiority when compared both with state-of-the-art and standard \DE{}.
%
Additionally, standard \DE{} attained quite promising results.
%
In fact it attained the third and second places in the problems of the \CEC{} 2016 and \CEC{} 2017 respectively.
%
Whereas that the proposed modification of DE attained the first places in both years.
%
This implies that the performance of the state-of-the-art algorithms is not so good in long-term executions.
%
%Specifically, although that in \CEC{} 2017 the L-SHADE-Epsilon algorithm got the lowest number of wins in the statistical test it showed a competitive score.
%
%This might occurs since that the statistical scores considers both mean and median errors.
%
%Morever, the score considers a rank and mean based on the error.
%

%TODO: Poner Dado que nuestra novedad está en el control de la diversidad, para comprender mejor el comportamiento de la propuesta...

Since our proposal is based on the explicit control of diversity, Fig. \ref{fig:diversity} shows the evolution of the mean of the diversity calculated
as the mean distance to the closest vector with the aim of better understanding its behavior.
%
Particularly, functions $f_1$ and $f_{30}$ were selected for this analysis because they have quite different features (easy uni-modal vs. complex multi-modal).
%
The left side shows the diversity of the Elite population.
%
It is remarkable that, while there are no direct constraints in the Elite population related to diversity, since it is generated from the population --- where constraints on diversity
are established --- the diversity is implicitly maintained.
%
The right side shows the diversity of the target vectors.
%
As expected, diversity decreases in a gradual way and a degree of diversity is maintained until the $90\%$ of the total function evaluations is reached.

\begin{figure}[t]
\centering
\begin{tabular}{cc}
   \includegraphics[scale=0.23, angle=-90]{img/Diversity_Elite.eps} 
   \includegraphics[scale=0.23, angle=-90]{img/Diversity_Target.eps} 
\end{tabular}
\caption{Mean distance to the closest vector of the 51 executions with the problems $f_1$ and $f_{30}$ (\CEC{} 2016 and \CEC{} 2017). The initial distance factor considered corresponds to $D_I=0.3$}
\label{fig:diversity}
\end{figure}



Finally, in order to provide comparable results of our proposal, Tables~\ref{tab:Results_CEC2016} and~\ref{tab:Results_CEC2017} report the best, worst, median, 
mean, standard deviation and success rate for both benchmarks.
%
These tables show that all the uni-modal problems were solved by our proposal.
%
Additionally, several simple and even some of the most complex multi-modal functions were optimally solved.
%
In fact, several complex functions that had never been solved by state-of-the-art could be solved by \DEEDM{}.
%
\begin{table}[t]
\begin{scriptsize}
\centering
\caption{Results for \DEEDM{} in the \CEC{} 2016 problems}
\label{tab:Results_CEC2016}
%\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & \textbf{Best} & \textbf{Worst} & \textbf{Median} & \textbf{Mean} & \textbf{Std.} & \textbf{Succ. Rate} \\ \hline
$f_1$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_2$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_3$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_4$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_5$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_6$ & 0.00E+00 & 3.60E-02 & 4.00E-03 & 7.39E-03 & 1.15E-02 & 3.92E-01 \\ \hline
$f_7$ & 2.00E-02 & 1.02E-01 & 5.90E-02 & 5.77E-02 & 4.93E-02 & 0.00E+00 \\ \hline
$f_8$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_9$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{10}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{11}$ & 0.00E+00 & 6.00E-02 & 0.00E+00 & 5.88E-03 & 1.90E-02 & 9.02E-01 \\ \hline
$f_{12}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{13}$ & 1.00E-02 & 8.00E-02 & 5.00E-02 & 4.67E-02 & 2.60E-02 & 0.00E+00 \\ \hline
$f_{14}$ & 1.00E-02 & 5.00E-02 & 3.00E-02 & 2.82E-02 & 2.13E-02 & 0.00E+00 \\ \hline
$f_{15}$ & 0.00E+00 & 4.70E-01 & 2.20E-01 & 1.99E-01 & 1.55E-01 & 1.96E-02 \\ \hline
$f_{16}$ & 4.00E-02 & 1.50E-01 & 8.00E-02 & 8.47E-02 & 4.96E-02 & 0.00E+00 \\ \hline
$f_{17}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{18}$ & 0.00E+00 & 2.00E-02 & 1.00E-02 & 7.65E-03 & 6.32E-03 & 3.14E-01 \\ \hline
$f_{19}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{20}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{21}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{22}$ & 0.00E+00 & 3.00E-02 & 0.00E+00 & 3.73E-03 & 2.76E-02 & 7.65E-01 \\ \hline
$f_{23}$ & 0.00E+00 & 1.00E+02 & 0.00E+00 & 2.55E+01 & 5.10E+01 & 7.45E-01 \\ \hline
$f_{24}$ & 0.00E+00 & 6.90E-01 & 0.00E+00 & 2.61E-02 & 1.33E-01 & 9.61E-01 \\ \hline
$f_{25}$ & 1.00E+02 & 1.00E+02 & 1.00E+02 & 1.00E+02 & 0.00E+00 & 0.00E+00 \\ \hline
$f_{26}$ & 8.00E-02 & 1.00E+02 & 5.29E+01 & 5.20E+01 & 3.19E+01 & 0.00E+00 \\ \hline
$f_{27}$ & 2.50E-01 & 9.10E-01 & 5.40E-01 & 5.60E-01 & 2.92E-01 & 0.00E+00 \\ \hline
$f_{28}$ & 0.00E+00 & 3.57E+02 & 3.43E+02 & 2.76E+02 & 1.60E+02 & 1.96E-01 \\ \hline
$f_{29}$ & 1.00E+02 & 1.00E+02 & 1.00E+02 & 1.00E+02 & 0.00E+00 & 0.00E+00 \\ \hline
$f_{30}$ & 1.84E+02 & 1.84E+02 & 1.84E+02 & 1.84E+02 & 3.25E-02 & 0.00E+00 \\ \hline
\end{tabular}%
%}
\end{scriptsize}
\end{table}

\begin{table}[t]
\begin{scriptsize}
\centering
\caption{Results for \DEEDM{} in the \CEC{} 2017 problems}
\label{tab:Results_CEC2017}
%\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & \textbf{Best} & \textbf{Worst} & \textbf{Median} & \textbf{Mean} & \textbf{Std.} & \textbf{Succ. Ratio} \\ \hline
$f_1$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_2$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_3$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_4$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_5$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_6$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_7$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_8$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_9$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{10}$ & 0.00E+00 & 1.20E-01 & 0.00E+00 & 1.65E-02 & 3.39E-02 & 7.45E-01 \\ \hline
$f_{11}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{12}$ & 0.00E+00 & 2.20E-01 & 0.00E+00 & 6.37E-02 & 1.76E-01 & 6.67E-01 \\ \hline
$f_{13}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{14}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{15}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{16}$ & 0.00E+00 & 2.10E-01 & 0.00E+00 & 2.47E-02 & 7.27E-02 & 8.82E-01 \\ \hline
$f_{17}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{18}$ & 0.00E+00 & 1.00E-02 & 0.00E+00 & 1.96E-03 & 4.47E-03 & 8.04E-01 \\ \hline
$f_{19}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{20}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{21}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{22}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{23}$ & 0.00E+00 & 3.00E+02 & 0.00E+00 & 3.49E+01 & 1.03E+02 & 8.82E-01 \\ \hline
$f_{24}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{25}$ & 0.00E+00 & 1.00E+02 & 0.00E+00 & 3.92E+00 & 2.00E+01 & 9.61E-01 \\ \hline
$f_{26}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{27}$ & 0.00E+00 & 3.87E+02 & 3.87E+02 & 2.05E+02 & 2.68E+02 & 1.96E-02 \\ \hline
$f_{28}$ & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 0.00E+00 & 1.00E+00 \\ \hline
$f_{29}$ & 1.45E+02 & 2.26E+02 & 2.18E+02 & 1.99E+02 & 4.21E+01 & 0.00E+00 \\ \hline
$f_{30}$ & 3.95E+02 & 3.95E+02 & 3.95E+02 & 3.95E+02 & 2.10E-01 & 0.00E+00 \\ \hline
\end{tabular}%
%}
\end{scriptsize}
\end{table}

\begin{figure}[t]
\centering
  \includegraphics[scale=0.6]{img/Tuning_CEC.eps}
\caption{Mean success rate for different $D_I$ in the \CEC{} 2016 and \CEC{} 2017 benchmarks with a population size equal to $250$ and $25 \times 10^6$ function evaluations}
\label{fig:one}
\end{figure}

\subsection{Validation of the Algorithms Belonging to the \textit{Replacement-Based} Category}

In this section the performance replacement strategy used by our proposal is compared with two popular replacement-based state-of-the-art strategies.
%
Particularly, the replacement-based methods taken into consideration are the \textit{Restricted Tournament Selection (\RTS{})} and \textit{ Hybrid Genetic Search with Adaptive Diversity Control  (\HGSADC{})}.
%
Those replacement-based strategies are incorpored to the stantandard \DE{}, therefore the \DE{} components are used, i.e. mutation, crossover and selection.
%
To have fairly comparison, the three variants are considered with the same parameterization.
%
Principally, the same guideline previously defined is taken into consideration.
%
In the tables \ref{tab:Summary_CEC2016_Replacement} and \ref{tab:Summary_CEC2017_Replacement} the results of the replacement-based strategies of each year are shown.%.
%
Specifically, the \DEEDM{} attainted the best results in the two years, also the second place is atteined by the \HGSADC{}.
%
However, the \DEEDM{} attained a significant highly $Score$ in relation with the remainint methods.
%
In General, the \RTS{} and \HGSADC{} attained bad results, this might occurs mainly for two aspects.
%
Firstly, the replacement-based strategies of the state-of-the-art only delays the premature convergency, as results bad quality solutions are attained.
%
Secondly, the incorporation of those methods in \DE{} seems to be contradictory with the greedy selection of the \DE{}, this drawback is not present in our replacement-strategy method.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\centering
\caption{Summary results replacement algorithms - \CEC{} 2016}
\label{tab:Summary_CEC2016_Replacement}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Always \\ solved\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}At least one\\ time solved\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
%\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{Always solved}} & \multirow{2}{*}{\textbf{At least one time solved}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
 &  &  & $\uparrow$ & $\downarrow$ & $\longleftrightarrow $ &  \\ \hline
\textbf{DE-EDM} & 13 & 21 & 51 & 1 & 8 & 100.00 \\ \hline
\textbf{RTS} & 2 & 7 & 3 & 47 & 10 & 19.74 \\ \hline
\textbf{HGSADC} & 3 & 15 & 21 & 27 & 12 & 44.12 \\ \hline
\end{tabular}
\end{table}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\centering
\caption{Summary results replacement algorithms - \CEC{} 2017}
\label{tab:Summary_CEC2017_Replacement}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Always \\ solved\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}At least one\\ time solved\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
%\multirow{2}{*}{\textbf{Algorithm}} & \multirow{2}{*}{\textbf{Always solved}} & \multirow{2}{*}{\textbf{At least one time solved}} & \multicolumn{3}{c|}{\textbf{Statistical Tests}} & \multirow{2}{*}{\textbf{Score}} \\ \cline{4-6}
 &  &  & $\uparrow$ & $\downarrow$ & $\longleftrightarrow $ &  \\ \hline
\textbf{DE-EDM} & 21 & 28 & 49 & 0 & 11 & 100.00 \\ \hline
\textbf{RTS} & 4 & 12 & 2 & 49 & 9 & 30.91 \\ \hline
\textbf{HGSADC} & 6 & 18 & 23 & 25 & 12 & 40.86 \\ \hline
\end{tabular}
\end{table}


\subsection{Empirical analyses of the initial distance factor}

In our proposal the diversity is explicitly promoted and the total amount of diversity maintained in the population 
depends on the initial distance factor $D_I$.
%
Therefore, the effect of this parameter on the quality is analyzed in this section.
%
Particularly, the same scheme previously mentioned was taken into account.
%Particularly, the same scheme taken into accoun in the previous analyses was taken into account.
%
However, several initial distance factors were considered ($D_I = \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1 \}$).


Fig. \ref{fig:one} shows the mean success rate attained for both benchmarks when considering different $D_I$ values.
%
The most relevant conclusions are:
\begin{itemize}
\item If diversity is not promoted ($D_I = 0.0 $) there is an important degradation in the performance. 
\item Performance is quite robust in the sense that a large range of $D_I$ values provide proper performance. For instance values
in the range $[0.2, 0.6]$ provide high-quality solutions.
\item If $D_I$ is too large (values larger than $0.6$), the quality of solutions decreases. This decrease of quality is gradual, i.e. as $D_I$ increases the quality slowly decreases.
\end{itemize}

To summarize, \DEEDM{} incorporates a novel parameter which is important for performance.
%
However, results are quite robust in the sense that even if it is not tuned for each problem, high-quality results can be obtained and that
a large range of values provide competitive results.

