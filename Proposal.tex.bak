Principally, our proposal\footnote{The code in C++ can be consulted in the next link \url{https://github.com/joelchaconcastillo/Diversity\_DE\_Research.git} } is  based in the following two works.
%
The first one that is delimited for DE algorithms, it is shown by Montgomery et al. \cite{montgomery2012simple} where is suggested a strategy to prevented the premature caused by the displacement of the mutation operator.
%
The second one that is a generalization of EAs, it is proposed by Carlos Segura et al. \cite{segura2016novel} and it transforms the single objective optimization problem to multiple objectives where one of them is the fitness and the other one is a diversity measurement, similarly is used a threshold which is decreased as the criteria stop is reached.
%

Particularly, our proposal induces a balance between exploration and exploitation that is automatically adjusted on the given stopping criterion.
%
Thus, the stopping criterion, as well as the elapsed time or the evaluations already executed, are used as inputs to the replacement strategy.
%
In this way, for shorter stoping criteria the method induces a faster reduction in diversity than for longer stopping criteria.
%
To achieve such balance are considered three populations, parent vectors, offspring vectors and elite vectors, being one of the novelties of the new design.
%

One of the basic principles behind the development of the replacement strategy devised in this paper is that individuals that contribute too little to diversity --the contribution is measured with the Distance to Closest Neighbour (DCN) value-- should not be accepted as be part of the parent vectors, instead it could replace one of the elite vectors.
%

In our approach, the vectors that contribute too little to the diversiy are penalized.
%
The value $D_t$\footnote{Do not confuse the threshold distance $D_t$ with the dimension $D$. } represent the minimum DCN required to avoid being penalized.
%
Any vector whose DCN value is lower than this threshold value is penalized.
%
The key principle resides in how to evaluate wheter an vector contributes enough or not, i.e., how to set the value $D_t$.
%
The value of $D_t$ should depend on the optimization stage.
%
Specifically, this value should be reduced as the stopping criterion is approached.
%
In our scheme, an initial $D_I$ value must be set.
%
Then, a linear reduction of $D_t$ is done.
%
Particularly, in this work, the stopping criterion is set by function evaluations (nfes).
%
The reduction is calculated in such way that by the $95\%$ of maximum number of evaluations the resulting $D_t$ value is $0$, and the rest is present a similar behaviour of the classical DE.
%
Thus, if $max\_nfes$ is the maximum number of evaluations and $nfes$ the elapsed number of evaluations, $D_t$ can be calculated as $D_t=D_I - D_I *(nfes/(0.95*max\_nfes))$.
%
According to Segura et al. \cite{segura2016novel} updating $D_t$ is more appropiate through a linear reduction.
%


%
Specifically, the previously strategy is implemented in the replacement phase (algorithm \ref{alg:Replacement}) where is used a popular niche-strategy known as \textit{Speciation} \cite{yang2017multimodal}.
%
Initially, based in a niche-radius ($D_t$) and a defined distance\footnote{For simplicity we use euclidean distance, however can be user other distance as the mahalanobis distance.} (equation \ref{eqn:distance}), in an iterative process the seeds (or survivors) are identified, these are the vectors with best fitness and whose minimal DCN is not lower than the one determined by the $D_t$ value.
%
It is important to remark that should be considered the normalized distance in such way that each dimension is equally important and the maximum distance is the unity, and as is suggested in previous works the initial niche-radius ($D_I$) is the fraction of the main space diagonal.

\begin{equation}\label{eqn:distance}
distance ( x_{seed}, x_j ) = \frac{\sqrt{ \sum_{d=1}^D \left ( \frac{x_{seed}^d - x_j^d}{max_d - min_d} \right )^2  }} {\sqrt{D}}
\end{equation}

%
Therefore, the vectors that have a lowest distance to any seed than $D_t$ are moved to the penalized set.
%
In this way are preserved the best fitness vectors and simultaneously the diversity is maintained in some level.
%
It is important to take into account that if the niche-radio is too high, just one seed or survivor will be selected.
%
In this scenario the rest of parent vectors are selected from the penalized vectors.
%
Thus, are selected the penalize vectors that considering the selected seeds vectors have the maximum constribution to diversity.
%
Although that in the literature exist several diversity measures, we consider the DCN.
%
According this, in an iterative process is selected as survivor the penalized vector that has the maximum DCN.

\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{Replacement Phase} \label{alg:Replacement}
	\begin{algorithmic}[1]
	\STATE $Survivors = Penalized = \emptyset$.
	\STATE $Current = Population \cup Offspring \cup Elite$.
	\STATE Sort $Current$ according to fitness.
	\WHILE{ $Survivors < pop\_size$ }
	   \STATE Select the best individual $Current_{best}$ of $Current$ as a new seed.
	   \STATE Find the other individuals nearest according to Eq. (\ref{eqn:distance}) and move to $Penalized$.
	   \STATE Move the best individual $Current_{best}$ to $Survivors$.
	\ENDWHILE
	\WHILE{ $Survivors < pop\_size$ }
	   \STATE Select the individual $Penalized$ with maximum distance to closest $Survivor$.
	   \STATE Move individual $Penalized$ to $Survivors$.
	\ENDWHILE
       \RETURN $Current$
\end{algorithmic}
\end{algorithm}

On the other hand, since that the diversity in the parent vectors should be kept, the selection operator indicated in the equation (\ref{eqn:selection}) is modified.
%
Thus, instead of made a comparison between the target or parent vectors and the trial or offspring vectors, is applied a comparison between the offspring vectors with the elite vectors.
%
Hence, the elite vectors record the best individuals obtained among the optimization process.
%


\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{General scheme of DE considering diversity} 
	\begin{algorithmic}[1]
	\STATE Randomly initialize the population of $NP$ individuals, where each one is uniformly distirbuited.
	\STATE Update $D_t = D_I - D_I *(nfes/(0.95*max\_nfes)) $ 
	\WHILE{ stopping criterion is nor satisfied}
	   \FOR{ $i=1$ to $NP$} 
		\STATE Mutation: Generate the donor vector according Eq. (\ref{eqn:mutation})
		\STATE Crossover: Recombine the mutate vector according Eq. (\ref{eqn:crossover})
		\STATE Selection: Update the parent vector according  Eq. (\ref{eqn:selection})
		\STATE Replacement: Select the parent vectors according to algorithm \ref{alg:Replacement}
	   \ENDFOR
	\ENDWHILE
    \label{alg:Fase_Remplazo_VSD-MOEAD}
\end{algorithmic}
\end{algorithm}


An advantage of our proposal is that it alliviates one critical weakness of the DE algorithms.
%
These are the control parameters both crossover probability ($CR$) and mutation factor ($F$).
%
Based in several studies showed by Montgomery et al. \cite{montgomery2010analysis}, $CR$ is perhaps the most important.
%
Extremes $CR$ values leads to vastly different search behaviours.
%
Low values of $CR$ result in a search that is not just aligned with a small number of search space axes, but which is gradual, slow and robust.
%
High values of $CR$ result in searches where fewer generated solutions may be improving, but the improvements can be large.
%
According this, we employ both high and low $CR$ values showed in the equation \ref{eqn:cr}.

\begin{equation} \label{eqn:cr}
CR = 
\begin{cases}
     Norm(0.2, 0.1),& \text{if} \quad rand[0,1] \leq 0.5  \\
     Norm(0.9, 0.1),              & \text{otherwise}
\end{cases}
\end{equation}


On the other hand, the mutation factor $F$ is computed as follows.
%
For each vector is sampled a $F$ value with a Cauchy distribution $Cauchy(0.5, 0.5*nfes/max\_nfes)$.
%
In this way the shape of the distribution increases with the function evaluations and therefore are generated more extremal values at the end of execution, this aims avoid stagnation in differents stages of the algorithm.
%
