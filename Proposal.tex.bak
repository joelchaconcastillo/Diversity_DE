In general, our proposal is motivated by two remarkable works in the area of diversity-based EAs.
%
First, the empirical studies of premature convergence developed by Montgomery et al. \cite{montgomery2012simple}.
%
They stablished several empirical analyzes to diagnose the premature convergence that appear in \DE{}.
%
The second work, provides significant improvements and a solid literature revision in the combinatorial optimization field, it was considered by Segura et al.~\cite{segura2016novel}.
%
Particularly, they incorporated a novel replacement strategy called \textit{The Replacement with Multiobjective based Dynamic Diversity Control} (\RMDDC{}) to relate the control of diversity with the stopping criterion and elapsed generations.
%
Important benefits were attained by methods including \RMDDC{}, so given the conclusions of these previous works, the proposal of this paper is a 
novel \DE{} variant that includes an explicit mechanism that follows similar principles to \RMDDC{}.
%
This novel optimizer is called \textit{Differential Evolution with Enhanced Diversity Maintenance} (\DEEDM{}) and its source
code is freely available~\footnote{The code in C++ can be downloaded in the next link \url{https://github.com/joelchaconcastillo/Diversity\_DE\_Research.git}}.

The main novelty of \DEEDM{} is the incorporation of a replacement strategy, which is similar to the one used in \RMDDC{}, i.e.
individuals that contribute too little to diversity should not be accepted as target vectors of the next generation.
%
At the same time, the solutions of high quality are recorded in the elite population.
%
Moreover, in order to establish the minimum acceptable diversity contribution the stopping criterion and elapsed
generations are taken into account.

The contribution to diversity is estimated with the Distance to Closest Neighbour metric (\DCN{}), i.e. given
a set of already selected individuals ($Survivors$), the contribution of a non-selected one is calculated as the minimum
distance to any of the individuals in $Survivors$.
%
However, in order to promote a faster convergence than in \RMDDC{} two modifications are performed.
%
First, no concepts of the multi-objective field are applied, instead a more greedy selection is taken into account.
%
Second, a new population called the elite population is cosidered, thus the best solutions with the worst diversity level are considered among the execution.

\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{General scheme of DE-EDM} 
	\begin{algorithmic}[1]
	\STATE Randomly initialize the population of $NP$ individuals, where each one is uniformly distributed.
	\STATE $G=0$
	\WHILE{ stopping criterion is not satisfied}
	   \FOR{ $i=1$ to $NP$} 
		\STATE Mutation: Generate the donor vector ($V_{i,G}$) according Eq. (\ref{eqn:mutation}).
		\STATE Crossover: Recombine the mutate vector ($U_{i,G}$) according Eq. (\ref{eqn:crossover}).
		\STATE Selection: Update the elite vector ($E_{i,G}$ instead of $X_{i,G}$) according Eq. (\ref{eqn:selection}).
		\STATE Replacement: Select the target vectors ($X_{i,G+1}$) according to Algorithm \ref{alg:Replacement} .
	   \ENDFOR
	   \STATE $G=G+1$
	\ENDWHILE
    \label{alg:Fase_Remplazo_VSD-MOEAD}
\end{algorithmic}
\end{algorithm}



\RMDDC{} is a replacement strategy that considers the maximization of the diversity 
contribution of each individual as an explicit objective.
%
Then, the notion of Pareto dominance is used to select the survivors.
%
It uses a dynamic threshold to prevent the selection of individuals with low contribution to diversity.
%
One of the main weaknesses of this method is that its convergence is highly delayed.
%
Thus, executions of several days were required to attain high-quality solutions.
%
As a result our proposal incorporates two extensions to alleviate such a drawback.
%
\DEEDM{} alters the replacement strategy of \DE{} with the aim of controlling the 
balance between exploration and exploitation.
%
This modification considers the inclusion of an elite population, which records the individuals with the best fitness.
%

Our replacement strategy (see Algorithm \ref{alg:Replacement}) operates as follows.
%
It receives as input the parent population (target vectors), the offspring population (trial vectors), and the elite population.
%
In each generation it must select the next parent population (target vectors).
%
Additionally, it calculates the desired minimum distance $D_t$ given the current number of function evaluations.
%
First, it joins the three populations in a set of current members.
%
The current members set contains vectors that might be selected to survive.
%
Then, the set of survivors and penalized individuals are initialized to the empty set.
%
In order to select the $NP$ survivors (target vectors) an iterative process is repeated.
%
In each step the best individual in the \textit{Current set}, i.e. the one with best objective function is selected
to survive (i.e. moved to the \textit{Survivor set}).
%
Then, the individuals in the \textit{Current set} with a lower metric \DCN{} than $D_t$ are transferred to the \textit{Penalized set}.
%
The \DCN{} metric taken into account is between each one \textit{Current set} to its closest selected individual.
%
In case were the \textit{Current set} is empty previous to the selection of $NP$ individuals, the \textit{Survivor set} is filled by selecting in each step the Penalized individuals with the largest \DCN{} to the \textit{Survivor set}.


\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{Replacement Phase} \label{alg:Replacement}
	\begin{algorithmic}[1]
	\STATE Input: $Population$ ($target$ $vectors$), $Offspring$ ($trial$ $vectors$), and $Elite$
	\STATE Update $D_t = D_I - D_I *(nfes/(0.95*max\_nfes)) $ 
	\STATE $Current = Population \cup Offspring \cup Elite$.
	\STATE $Survivors = Penalized = \emptyset$.
	\WHILE{ $|Survivors| < NP$ OR $|Current| > 0$ }
	   \STATE Select the best individual ($Current_{best}$) of $Current$.
	   \STATE Find the individuals with lowest \DCN{} to $D_t$ between $Current$ and $Current_{best}$ and move them to $Penalized$ (considering the normalized distance of Eq. (\ref{eqn:distance})).
	   \STATE Move the best individual $Current_{best}$ to $Survivors$.
	\ENDWHILE
	\WHILE{ $|Survivors| < pop\_size$ }
	   \STATE Select the individual $Penalized$ with maximum \DCN{}.
	   \STATE Move the individual from $Penalized$ to $Survivors$.
	\ENDWHILE
       \RETURN $Survivors$
\end{algorithmic}
\end{algorithm}


In order to complete the description is important to specify the way to calculate $D_t$ and the methodology to update the 
elite individuals.
%
The remaining steps are maintained as in the classic \DE{} variant.
%
The value of $D_t$ should depend on the optimization stage.
%
Specifically, this value should be reduced as the stopping criterion is reached.
%
In our scheme, an initial $D_I$ value must be set.
%
Then, a linear reduction of $D_t$ is done.
%
Particularly, in this work, the stopping criterion is set by function evaluations (\textit{nfes}).
%
The reduction is calculated in such way that by the $95\%$ of maximum number of evaluations the resulting $D_t$ value is $0$.
%
Therefore, in the remaining $5\%$ the behavior is similar to the classical DE.
%
Thus, if $max\_nfes$ is the maximum number of evaluations and $nfes$ is the elapsed number of evaluations, $D_t$ can be calculated as $D_t=D_I - D_I *(nfes/(0.95*max\_nfes))$.
%
The reduction of this parameter is based in a linear reduction, as is indicated in \cite{segura2016novel}.
%
They indicated that a linear decrement generally provides the most stable results.
%%
%%We consider a linear model reduction based in empirical studies of similar works \cite{segura2016novel}, where they indicated that a linear decrement generally provides the most stable results.
%%%

Principally, the replacement phase considers a defined radius ($D_t$), which for simlicity is considered through the normalized euclidean distance (Eq. \ref{eqn:distance}), however other distances could be implemented (e.g. mahalanobis distance).
%
It is important to mention that the normalized distance is needed, thus each dimension is equally important, also this simplifies the setting of the $D_I$ parameter, since that the maximum distance is the unity, which is a fraction of the main space diagonal.

\begin{equation}\label{eqn:distance}
distance ( x_{seed}, x_j ) = \frac{\sqrt{ \sum_{d=1}^D \left ( \frac{x_{seed}^d - x_j^d}{max_d - min_d} \right )^2  }} {\sqrt{D}}
\end{equation}


%
Theoretically, the effect of the initial distance factor ($D_I$) affects directively the behavior of the algorithm.
%
If this parameter is fixed large enough, then at the first optimization stages the algorithm aims to maximize the \DCN{}, several difficult problems might required high diversity level to properly obtain quality solutions (e.g. deceptive problems).
%
Also, the solutions should be disperse if the target vector are initiallt in the same region of the decision variables.
%
Thus, this effect alliviate the premature convergence in several scenarios.
%
On the other hand, a low initial distance factor results in a lower level exploration, therefore it might provoke exploitation inside of several basis of attraction.
%
Depending in the optimization stage and the fitness landscape it could be benefical, principally in multi-modal problems, where each basis of attraction seems to be promising and each one of them should have the suitable quantity of computationals efforts.
%
It is interesting to take into account that our proposal could be traslated to the multi-modal fashion just setting a final distance factor, which should be strictly positive, however this trend is not considered in this work.
%
Therefore, when the initial distance factor is setted several factors should be taken into account.
%
One of them is the maximum number of evaluations, when the problem is setted with few number of evaluations, the initial distance factor should be low, since the exploitation should be promoted.
%
On the other hand, when the problem is configured with a large number of evaluations, then the initial distance factor should be higher.
%
Also, the number of vectors (population size) should be considered, since that they are directly related with the diversity.
%
Although our proposal should take into account the previously details, there exist an usual range where the results are enough stable, it is showed in the empirical analyzes.
%

Our proposal provides several advantages in constrast of the standard DE and some of the state-of-the-art algorithms, perhaps the most important is that it is not over-parametrized.
%
Although several top-rank algorithms present good enough results, usually these algorithms need a tunning phase to fit the parameters, therefore in real application it could be infeasible.

%
Specifically, the standard DE recives two parameters, these are the crossover probability ($CR$) and the mutation factor ($F$).
%
The first one is perhaps the most important according to several studies showed by Montgomery et al. \cite{montgomery2010analysis}.
%
These authors empirically proved that extremes $CR$ values leads to vastly different search behaviors.
%
They explained that low $CR$ values result in a search that is not just aligned with a small number of search space axes, it also shows small displacements, which provokes a gradual and slow convergence that in some scenarios migh result in a robust behavior.
%
On the other hand, high $CR$ values might generate few quality solutions, however these solutions provoke large displacements that could improve significantly the solutions.
%
According this, we employ both high and low $CR$ values as is showed in the equation \ref{eqn:cr}.

\begin{equation} \label{eqn:cr}
CR = 
\begin{cases}
     Normal(0.2, 0.1),& \text{if} \quad rand[0,1] \leq 0.5  \\
     Normal(0.9, 0.1),              & \text{otherwise}
\end{cases}
\end{equation}


In the same vein, the mutation factor $F$ is computed as follows.
%
For each vector is sampled a $F$ value with a Cauchy distribution $Cauchy(0.5, 0.5*nfes/max\_nfes)$.
%
Particularly, it is based in several empirical results, since that at the first optimization stages the density function is located in $0.5$.
%
While the execution transcurs, the density function changes and the values generated are closest to $0.0$ and to $1.0$.
%
This occurs since the Cauchy density function has heavly tails, therefore the values that are generated outside the range $[0.0, 1.0]$ are truncated.
%
Thus, more extremal $F$-values are stimulated and a vastly exploration is obtained.
%
% thus the shape of the distribution increases with the function evaluations and therefore are generated more extreme values at the end of execution, this aims avoid stagnation in different stages of the algorithm.
%


Thus, briefly our proposal is based in several ideas of the mentioned works, and are listed as follows:
\begin{itemize}
\item Is considered a threshold to control explicitly the convergence of the solutions.
\item This threshold decreases over the algorithm's run.
\item The selection operator is relaxed in the sense that it does not provokes premature convergence, this is attained considering an elite population. 
\end{itemize}

