Our proposal is motivated by two main works in the area of diversity-based EAs.
%
First, the empirical studies developed by Montgomery et al~\cite{montgomery2012simple},
which presents several empirical analyses to confirm issues related to premature convergence in \DE{}.
%
The second work, by Segura et al.~\cite{segura2016novel}, provides significant improvements in the combinatorial optimization field.
%
Particularly, they incorporated a novel replacement strategy called \textit{The Replacement with Multi-objective based Dynamic Diversity Control} (\RMDDC{}) 
to relate the control of diversity with the stopping criterion and elapsed generations.
%
Important benefits were attained by methods including \RMDDC{}, so given the conclusions of these previous works, the proposal of this paper is a 
novel \DE{} variant that includes an explicit mechanism that follows similar principles to \RMDDC{}.
%
This novel optimizer is called \textit{Differential Evolution with Enhanced Diversity Maintenance} (\DEEDM{}) and its source
code is freely available~\footnote{The code in C++ can be downloaded in the next link \url{https://github.com/joelchaconcastillo/Diversity\_DE\_Research.git}}.

The novelty of \DEEDM{} (see Algorithm~\ref{alg:DEEDM}) is the incorporation of a replacement strategy that follows some of the principles of the one used in \RMDDC{}, i.e.
individuals that contribute too little to diversity should not be accepted as members of the next generation.
%
In this way, the greedy selection strategy of \DE{} is not used to maintain the parent population ($X$).
%
However, an additional population, which is called the elite population ($E$), is incorporated and this one is managed in a similar way than 
the population of \DE{}, i.e. with a greedy replacement.
%
Moreover, in order to establish the minimum acceptable diversity contribution to be selected, the stopping criterion and elapsed
generations are taken into account.
%
One of the main weaknesses of this method is that its convergence is highly delayed.
%
Thus, in order to promote a faster convergence than in \RMDDC{} two modifications are performed.
%
First, no concepts of the multi-objective field are applied, instead a more greedy selection is taken into account.
%
Second, the elite population is also considered as an input of the replacement strategy.

\begin{algorithm}[t]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{General scheme of DE-EDM} 
	\begin{algorithmic}[1]
	\STATE Randomly initialize the population of $NP$ individuals, where each one is uniformly distributed.
	\STATE $G=0$
	\WHILE{ stopping criterion is not satisfied}
	   \FOR{ $i=1$ to $NP$} 
		\STATE Mutation: Generate the donor vector ($V_{i,G}$) according Eq. (\ref{eqn:mutation}).
		\STATE Crossover: Recombine the mutate vector ($U_{i,G}$) according Eq. (\ref{eqn:crossover}).
		\STATE Selection: Update the elite vector ($E_{i,G}$ instead of $X_{i,G}$) according Eq. (\ref{eqn:selection}).
		\STATE Replacement: Select the target vectors ($X_{i,G+1}$) according to Algorithm \ref{alg:Replacement} .
	   \ENDFOR
	   \STATE $G=G+1$
	\ENDWHILE
    \label{alg:DEEDM}
\end{algorithmic}
\end{algorithm}

%\DEEDM{} alters the replacement strategy of \DE{} with the aim of controlling the 
%balance between exploration and exploitation by extending \RMDDC{}.
%
%The principles of the \RMDDC{} are as follows.
%
%It considers the maximization of the diversity contribution of each individual as an explicit objective.
%
%Then, the notion of Pareto dominance is used to select the survivors.
%
%It uses a dynamic threshold to prevent the selection of individuals with low contribution to diversity.
%
%
%Thus, executions of several days were required to attain high-quality solutions in many of the tested problems.
%
%As a result our proposal incorporates two extensions to alleviate such a drawback.
%
%This modification considers the inclusion of an elite population, which records the individuals with the best fitness.
%

Our replacement strategy (see Algorithm \ref{alg:Replacement}) operates as follows.
%
It receives as input the parent population (target vectors), the offspring population (trial vectors), and the elite population.
%
In each generation it must select the $NP$ vectors of the next parent population.
%
First, it calculates the desired minimum distance $D_t$ given the current number of elapsed function evaluations (line 2).
%
Then, it joins the three populations in a set of current members (line 3).
%
The current members set contains vectors that might be selected to survive.
%
Then, the set of survivors and penalized individuals are initialized to the empty set (line 4).
%
In order to select the $NP$ survivors (next parent population) an iterative process is repeated (lines 5 - 13).
%
In each step the best individual in the \textit{Current set}, i.e. the one with best objective function is selected
to survive, i.e. it is moved to the \textit{Survivor set} (line 6 - 8).
%
Then, individuals in the \textit{Current set} with a distance metric lower than $D_t$ are transferred to the \textit{Penalized set} (line 9).
%
The way to calculate the distance between two individuals is by a using a normalized Euclidean distance (Eq.~\ref{eqn:distance}).
%
%TODO: explicar la f√≥mrula
%
In cases where the \textit{Current set} is empty previous to the selection of $NP$ individuals, the \textit{Survivor set} is filled by selecting in each step 
the individual in $Penalized$ with the largest distance to the closest individual in the \textit{Survivor set}.

\begin{equation}\label{eqn:distance}
distance ( x_{i}, x_j ) = \frac{\sqrt{ \sum_{d=1}^D \left ( \frac{x_{i}^d - x_j^d}{max_d - min_d} \right )^2  }} {\sqrt{D}}
\end{equation}


\begin{algorithm}[t]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{Replacement Phase} \label{alg:Replacement}
	\begin{algorithmic}[1]
	\STATE Input: $Population$ ($target$ $vectors$), $Offspring$ ($trial$ $vectors$), and $Elite$
	\STATE Update $D_t = D_I - D_I *(nfes/(0.95*max\_nfes)) $ 
	\STATE $Current = Population \cup Offspring \cup Elite$.
	\STATE $Survivors = Penalized = \emptyset$.
	\WHILE{ $|Survivors| < NP$ And $|Current| > 0$ }
	   \STATE $Selected$ = Select the best individual of $Current$.
		 \STATE Remove $Selected$ from $Current$.
	   \STATE Copy $Selected$ to $Survivors$.
	   \STATE Find the individuals from $Current$ with a distance to $Selected$ lower than $D_t$ and move them to $Penalized$. Normalized distance is considered (Eq. \ref{eqn:distance}).
	\ENDWHILE
	\WHILE{ $|Survivors| < NP$ }
	   \STATE $Selected$ = Select the individual from $Penalized$ with the largest distance to the closest individual in $Survivors$.
		 \STATE Remove $Selected$ from $Penalized$.
	   \STATE Copy $Selected$ to $Survivors$.
	\ENDWHILE
  \RETURN $Survivors$
\end{algorithmic}
\end{algorithm}


In order to complete the description is important to specify the way to calculate $D_t$ and the methodology to update the 
elite individuals.
%
All the remaining steps are maintained as in the classic \DE{} variant.
%
The value of $D_t$ is used to alter the degree between exploration and explotation so it should depend on the optimization stage.
%
Specifically, this value should be reduced as the stopping criterion is reached with the aim of promoting explotation.
%
In our scheme, an initial value for $D_t$ ($D_I$) must be set.
%
Then, similarly than in~\cite{segura2016novel}, a linear reduction of $D_t$ is performed by taking into account the elepased function evaluations and stopping criterion.
%
Particularly, in this work, the stopping criterion is set by function evaluations (\textit{nfes}).
%
The reduction is calculated in such way that by the $95\%$ of maximum number of evaluations the resulting $D_t$ value is $0$.
%
Therefore, in the remaining $5\%$ diversity is not considered at all.
%
Thus, if $max\_nfes$ is the maximum number of evaluations and $nfes$ is the elapsed number of evaluations, $D_t$ can be calculated as $D_t=D_I - D_I *(nfes/(0.95*max\_nfes))$.
%
%The reduction of this parameter is based in a linear reduction, as is indicated in .
%
%They indicated that a linear decrement generally provides the most stable results.
%%
%%We consider a linear model reduction based in empirical studies of similar works \cite{segura2016novel}, where they indicated that a linear decrement generally provides the most stable results.
%%%

%Principally, the replacement phase considers a defined radius ($D_t$), which for simplicity is considered through the normalized euclidean distance (Eq. \ref{eqn:distance}), however other distances could be implemented (e.g. mahalanobis distance).
%
%It is important to mention that the normalized distance is needed, thus each dimension is equally important, also this simplifies the setting of the $D_I$ parameter, since that the maximum distance is the unity, which is a fraction of the main space diagonal.


The initial distance ($D_I$) heavily affects the performance of \DEEDM{}.
%
If this parameter is fixed large enough, then at the first optimization stages the algorithm aims to maximize the diversity of the population, 
so a proper exploration is performed which is very important in some kinds of problems such as highly multimodal and deceptive ones.
%
Thus, the effect of premature convergence might be alleviated.
%
A too large $D_I$ might induce too much exploration so a proper exploitation phase is not performed.
%
In the opposite case, a too low $D_I$ might avoid the exploration phase, so avoiding local optima is more difficult.
%
Depending on the kind of fitness landscape and stopping criterion, the optimal $D_I$ might vary.
%
For instance, deceptive and highly multi-modal problems usually require larger values than unimodal problems.
%
However, in our proposal, $D_I$ is not adapted to each problem, instead an experimental study to check the robustness
of different $D_I$ value is attached in the experimental validation section. 
%
%It is interesting to take into account that our proposal could be translated to the multi-modal fashion just setting a final distance factor, which should be strictly positive, however this trend is not considered in this work.
%
%Therefore, when the initial distance factor is setted several factors should be taken into account.
%
%One of them is the maximum number of evaluations, when the problem is setted with few number of evaluations, the initial distance factor should be low, since the exploitation should be promoted.
%
%On the other hand, when the problem is configured with a large number of evaluations, then the initial distance factor should be higher.
%
%Also, the number of vectors (population size) should be considered, since that they are directly related with the diversity.
%
%Although our proposal should take into account the previously details, there exist an usual range where the results are enough stable, it is showed in the empirical analyzes.
%
%Our proposal provides several advantages in contrast of the standard DE and some of the state-of-the-art algorithms, perhaps the most important is that it is not over-parameterized.
%
%Although several top-rank algorithms present good enough results, usually these algorithms need a tuning phase to fit the parameters, therefore in real application it could be infeasible.

%
Similarly that the standard \DE{}, in \DEEDM{} the crossover probability ($CR$) and the mutation factor ($F$) must be set.
%
The first one is perhaps the most important for the performance according to several studies developed by Montgomery 
et al. \cite{montgomery2010analysis}.
%
These authors empirically proved that extremes $CR$ values leads to vastly different search behaviors.
%
They explained that low $CR$ values result in a search that is aligned with a small number of search space axes and
induce small displacements.
%
This provokes a gradual and slow convergence that in some scenarios might result in a robust behavior.
%
Additionally, high $CR$ values might generate quality solutions with a lower probability.
%
However, these transformations provoke large displacements that could improve significantly the solutions.
%
According to this, we employ both high and low $CR$ values as it is showed in eq. \ref{eqn:cr}.

\begin{equation} \label{eqn:cr}
CR = 
\begin{cases}
     Normal(0.2, 0.1),& \text{if} \quad rand[0,1] \leq 0.5  \\
     Normal(0.9, 0.1),              & \text{otherwise}
\end{cases}
\end{equation}


%TODO: reescribir
Following the principles of SHADE, the mutation factor $F$ is computed as follows. %TODO: poner cita a un SHADE que lo haga similar
%
For each vector, $F$ is sampled with a Cauchy distribution $Cauchy(0.5, 0.5*nfes/max\_nfes)$.
%
Particularly, at the first optimization stages the density function is located in $0.5$.
%
While the execution elapses, the density function changes and the values generated are closest to $0.0$ and to $1.0$.
%
This occurs since the Cauchy density function has heavily tails, therefore the values that are generated outside the range $[0.0, 1.0]$ are truncated.
%
Thus, more extreme $F$-values are stimulated and a vastly exploration is obtained.
%
% thus the shape of the distribution increases with the function evaluations and therefore are generated more extreme values at the end of execution, this aims avoid stagnation in different stages of the algorithm.
%


%Thus, briefly our proposal is based in several ideas of the mentioned works, and are listed as follows:
%\begin{itemize}
%\item Is considered a threshold to control explicitly the convergence of the solutions.
%\item This threshold decreases over the algorithm's run.
%\item The selection operator is relaxed in the sense that it does not provokes premature convergence, this is attained considering an elite population. 
%\end{itemize}

