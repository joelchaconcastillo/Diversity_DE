Mainly our proposal is motivated by two remarkable works in the area of diversity-based EAs.
%
Firstly, the premature convergence studies developed by Montgomery et al. \cite{montgomery2012simple}.
%
They stablished several empirical studies to diagnose the prematura convergence that appear in \DE{}.
%
Secondly, providing significant improvements and solid review of the EAs in combinatorial optimization field were considered Segura et al.~\cite{segura2016novel}.
%
Particularly, they incorporated a novel replacement strategy which is called the replacement with multiobjective based dynamic diversity control strategy (\RMDDC{}) to relate the control of diversity with the stopping criterion and elapsed generation.
%
Important benefits were attained by methods including \RMDDC{}, so given the conclusions of these previous works, the proposal of this paper is a 
novel \DE{} variant that includes an explicit mechanism that follows similar principles to \RMDDC{}.
%
This novel optimizer is called Differential Evolution with Enhanced Diversity Maintenance (\DEEDM{}) and its source
code is freely available~\footnote{The code in C++ can be downloaded in the next link \url{https://github.com/joelchaconcastillo/Diversity\_DE\_Research.git}}.

The main novelty of \DEEDM{} is the incorporation of a replacement strategy.
%
The main principle of the novel replacement strategy is similar to the one used in \RMDDC{}, i.e.
individuals that contribute too little to diversity should not be accepted as target vectors of the next generation.
%
At the same time, the solutions of high quality are recorded in the elite population.
%
Moreover, in order to establish the minimum acceptable diversity contribution the stopping criterion and elapsed
generations are taken into account.

The contribution to diversity is estimated with the Distance to Closest Neighbour metric (\DCN{}), i.e. given
a set of already selected individuals ($Survivors$), the contribution of a non-selected one is calculated as the minimum
distance to any of the individuals in $Survivors$.
%
However, in order to promote a faster convergence than in \RMDDC{} two modifications are performed.
%
First, no concepts of the multi-objective field are applied, instead a more greedy selection is taken into account.
%
Second, a new population called the elite population is used.

\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{General scheme of DE-EDM} 
	\begin{algorithmic}[1]
	\STATE Randomly initialize the population of $NP$ individuals, where each one is uniformly distributed.
	\STATE $G=0$
	\WHILE{ stopping criterion is nor satisfied}
	   \FOR{ $i=1$ to $NP$} 
		\STATE Mutation: Generate the donor vector ($V_{i,G}$) according Eq. (\ref{eqn:mutation}).
		\STATE Crossover: Recombine the mutate vector ($U_{i,G}$) according Eq. (\ref{eqn:crossover}).
		\STATE Selection: Update the elite vector ($E_{i,G}$ instead of $X_{i,G}$) according Eq. (\ref{eqn:selection}).
		\STATE Replacement: Select the target vectors ($X_{i,G+1}$) according to algorithm \ref{alg:Replacement} .
	   \ENDFOR
	   \STATE $G=G+1$
	\ENDWHILE
    \label{alg:Fase_Remplazo_VSD-MOEAD}
\end{algorithmic}
\end{algorithm}



\RMDDC{} is a replacement strategy that considers the maximization of the diversity 
contribution of each individual as an explicit objective.
%
Then, the notion of Pareto dominance is used to select the survivors.
%
It uses a dynamic threshold to prevent the selection of individuals with a too low contribution to diversity.
%
One of the main weaknesses of this method is that its convergence is highly delayed.
%
Thus, executions of several days were required to attain high-quality solutions.
%
As a result our proposal incorporates two extensions to alleviate such a drawback.

\DEEDM{} alters the replacement strategy of \DE{} with the aim of controlling the 
balance between exploration and exploitation.
%
%

Our replacement strategy (see Algorithm \ref{alg:Replacement}) operates as follows.
%
It receives as input the parent population (target vectors), the offspring population (trial vectors), and the elite population of the current generation and
it must select the next parent population (target vectors).
%
Additionally, it calculates the desired minimum distance $D_t$ given the current number of function evaluations.
%
First, it joins the three populations in a set of current members.
%
The current members set contains vectors that might be selected to survive.
%
Then, the set of survivors and penalized individuals are initialized to the empty set.
%
In order to select the $NP$ survivors (target vectors) an iterative process is repeated.
%
In each step the best individual in the \textit{Current set}, i.e. the one with best objective function is selected
to survive, i.e. moved to the \textit{Survivor set}.
%
Then, the individuals in the \textit{Current set} with minimum \DCN{} to the selected individuals lower than $D_t$ are transferred to the
\textit{Penalized set}.
%
In the case that the \textit{Current set} is empty previous to the selection of $NP$ individuals, the \textit{Survivor set} is filled by selecting in each step the Penalized individuals with the largest \DCN{} to the \textit{Survivor set}.


\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
  \scriptsize
	\caption{Replacement Phase} \label{alg:Replacement}
	\begin{algorithmic}[1]
	\STATE Input: $Population$ ($target$ $vectors$), $Offspring$ ($trial$ $vectors$), and $Elite$
	\STATE Update $D_t = D_I - D_I *(nfes/(0.95*max\_nfes)) $ 
	\STATE $Current = Population \cup Offspring \cup Elite$.
	\STATE $Survivors = Penalized = \emptyset$.
	\WHILE{ $|Survivors| < NP$ OR $|Current| > 0$ }
	   \STATE Select the best individual ($Current_{best}$) of $Current$.
	   \STATE Find the individuals with lowest \DCN{} to $D_t$ between $Current$ and $Current_{best}$ and move them to $Penalized$ (considering the normalized distance of Eq. (\ref{eqn:distance})).
	   \STATE Move the best individual $Current_{best}$ to $Survivors$.
	\ENDWHILE
	\WHILE{ $|Survivors| < pop\_size$ }
	   \STATE Select the individual $Penalized$ with maximum \DCN{}.
	   \STATE Move the individual from $Penalized$ to $Survivors$.
	\ENDWHILE
       \RETURN $Survivors$
\end{algorithmic}
\end{algorithm}


In order to complete the description is important to specify the way to calculate $D_t$ and the methodology to update the 
elite individuals.
%
The remaining steps are maintained as in the classic \DE{} variant.
%
The value of $D_t$ should depend on the optimization stage.
%
Specifically, this value should be reduced as the stopping criterion is approached.
%
In our scheme, an initial $D_I$ value must be set.
%
Then, a linear reduction of $D_t$ is done.
%
Particularly, in this work, the stopping criterion is set by function evaluations (nfes).
%
The reduction is calculated in such way that by the $95\%$ of maximum number of evaluations the resulting $D_t$ value is $0$.
%
Therefore, in the rest $5\%$ the behavior is similar to the classical DE.
%
Thus, if $max\_nfes$ is the maximum number of evaluations and $nfes$ the elapsed number of evaluations, $D_t$ can be calculated as $D_t=D_I - D_I *(nfes/(0.95*max\_nfes))$.
%
We consider a linear model reduction based in empirical studies of similar works \cite{segura2016novel}, where they indicated that a linear decrement provides the most stable results.
%

Principally, the replacement phase considers a defined radius ($D_t$), which for simlicity is considered through the normalized euclidean distance (eqn. \ref{eqn:distance}), however other distances could be implemented (e.g. mahalanobis distance).
%
It is important to mention that the normalized distance is needed, thus each dimension is equally important, also this simplifies to fix the $D_I$ parameter since tha maximum distance is the unity, which is a fraction of the main space diagonal.

\begin{equation}\label{eqn:distance}
distance ( x_{seed}, x_j ) = \frac{\sqrt{ \sum_{d=1}^D \left ( \frac{x_{seed}^d - x_j^d}{max_d - min_d} \right )^2  }} {\sqrt{D}}
\end{equation}


%
Theoretically, the effect of the initial distance factor ($D_I$) affects directively the behavior of the algorithm.
%
If this parameter is fixed too large, then essentially at the first optimization stages the algorithm aims to maximize the \DCN{}, thus the problems which need a high level of diversity that usually are the most difficult are properly solved (e.g. deceptive problems).
%
On the other hand, a low initial distance factor results in a lower level exploration, therefore it provokes a high level of exploitation.
%
Depending in the optimization stage and the fitness landscape it could be benefical, principally in multi-modal problems where each basis of attraction seems to be promising and each one of them should have the suitable quantity of computationals efforts.
%
It is interesting to take into account that our proposal could be traslated to the multi-modal fashion just setting a final distance factor, which should be strictly positive, however this trend is not considered in this work.
%
Therefore, when the initial distance factor is setted several factors should be taken into account.
%
One of them is the maximum number of evaluations, when the problem is setted with few number of evaluations, the initial distance factor should be lower, since the exploitation should be promoted.
%
On the other hand, when the problem is allowed with a large number of evaluations, then the initial distance factor should be higher, but also should be considered the number of vectors in the population, since that them are directly realted with the diversity.
%
Although our proposal should take into account the previously details, there exist an usual range where the results are enough stable, it is showed in the empirical analyzes.
%

An advantage of our proposal is that it alleviates one critical weakness of the DE algorithms.
%
These are the control parameters both crossover probability ($CR$) and mutation factor ($F$).
%
Based in several studies showed by Montgomery et al. \cite{montgomery2010analysis}, $CR$ is perhaps the most important.
%
Extremes $CR$ values leads to vastly different search behaviors.
%
Low values of $CR$ result in a search that is not just aligned with a small number of search space axes, but which is gradual, slow and robust.
%
High values of $CR$ migh generate few quality solutions, however these solutions provoke large displacements and could improve significantly.
%
According this, we employ both high and low $CR$ values as is showed in the equation \ref{eqn:cr}.

\begin{equation} \label{eqn:cr}
CR = 
\begin{cases}
     Norm(0.2, 0.1),& \text{if} \quad rand[0,1] \leq 0.5  \\
     Norm(0.9, 0.1),              & \text{otherwise}
\end{cases}
\end{equation}


On the other hand, the mutation factor $F$ is computed as follows.
%
For each vector is sampled a $F$ value with a Cauchy distribution $Cauchy(0.5, 0.5*nfes/max\_nfes)$.
%
In this way the shape of the distribution increases with the function evaluations and therefore are generated more extreme values at the end of execution, this aims avoid stagnation in different stages of the algorithm.
%


Our proposal is based in several ideas of the previously mentioned works, specifically the following:
\begin{itemize}
\item Is considered a threshold to control explicitly the convergence of the solutions.
\item This threshold decreases over the algorithm's run.
\item The selection operator is relaxed. 
\end{itemize}

